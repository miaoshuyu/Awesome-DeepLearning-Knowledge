# Awesome-DeepLearning-Knoweledge
#### 深度学习
* 代码写一下交叉熵损失函数：  
    https://blog.csdn.net/b1055077005/article/details/100152102  
    https://blog.csdn.net/francislucien2017/article/details/86763319

* 交叉熵损失函数原理详解    
https://blog.csdn.net/b1055077005/article/details/100152102

* 常见的损失函数:    
https://mp.weixin.qq.com/s/C45DxRB-n4zuxpzWrHdCzg

* sigmoid导数的大小范围：     
https://blog.csdn.net/qq_42648305/article/details/104271401

* 池化层（pooling）的反向传播（ReLu）        
https://blog.csdn.net/qq_21190081/article/details/72871704

* BN的公式及其含义作用等：      
 https://www.cnblogs.com/guoyaohua/p/8724433.html

* BN在inference的时候怎么加速  
  https://blog.csdn.net/qq_35985044/article/details/104609986  
https://blog.csdn.net/u012370185/article/details/98623718

* BN跨卡同步  
https://niecongchong.github.io/2019/08/17/Multi-GPU%E4%B8%8B%E7%9A%84Batch-normalize%E8%B7%A8%E5%8D%A1%E5%90%8C%E6%AD%A5/

* BN解决过拟合   
https://blog.csdn.net/qq_23150675/article/details/79452685

* layer Normalization介绍：      
   https://blog.csdn.net/liuxiao214/article/details/81037416

* BN和Dropout不能共用   
https://blog.csdn.net/songyunli1111/article/details/89071021

* Dropout训练和测试不同      http://www.360doc.com/content/18/1203/22/54525756_799102767.shtml

* Dropout和dropconnect的区别  
https://blog.csdn.net/zhangbaoanhadoop/article/details/82791308

* L1和L2     
https://blog.csdn.net/jinping_shi/article/details/52433975

* 一个框架看懂优化算法之异同 SGD/AdaGrad/Adam：  
https://zhuanlan.zhihu.com/p/32230623

* 标签平滑(Label smoothing)：    
https://www.cnblogs.com/itmorn/p/11254448.html

* 自集成和自蒸馏：  
  https://blog.csdn.net/weixin_43145361/article/details/106630873

* Focal Loss与GHM 解决样本不平衡利器     
https://zhuanlan.zhihu.com/p/80594704


* 感受野     
https://www.cnblogs.com/shine-lee/p/12069176.html   
https://blog.csdn.net/Kerrwy/article/details/82430530

* ROC和AUC    
https://www.cnblogs.com/gatherstars/p/6084696.html

----
#### 基础模型
* SeNet   
https://zhuanlan.zhihu.com/p/65459972

* MobileNets系列     
https://www.cnblogs.com/dengshunge/p/11334640.html

* ShuffleNet系列    
https://www.cnblogs.com/hellcat/p/10318630.html

* SqueezeNet系列   
https://zhuanlan.zhihu.com/p/49465950

* EfficientNet       
https://blog.csdn.net/u014380165/article/details/90812249

* GCNet and Non-local    
https://zhuanlan.zhihu.com/p/64988633

* Nottleneck Layer     
https://blog.csdn.net/zqnnn/article/details/88241852

* InceptionV1-V4   
https://www.cnblogs.com/haiyang21/p/7243200.html

* DenseNet    
https://blog.csdn.net/u014380165/article/details/75142664/

* GCN    
https://zhuanlan.zhihu.com/p/107162772

----
#### 目标检测

* NMS   
https://zhuanlan.zhihu.com/p/54709759

* SmoothL1/IoU/GIoU/DIoU/CIoU Loss   
https://zhuanlan.zhihu.com/p/104236411

* cross-entropy/focal-loss/GHM  
https://zhuanlan.zhihu.com/p/80594704

* Focal loss与 OHEM    
https://www.cnblogs.com/ymjyqsx/p/9508664.html

* 解决小目标检测！多尺度方法汇总  
https://zhuanlan.zhihu.com/p/141954282

* 密集物体检测   
https://blog.csdn.net/weixin_41876817/article/details/83054525

* Yolo系列  
https://blog.csdn.net/yuanlulu/article/details/89319839

* YoloV3里面darknet53模型特点，模型里面下采样用的什么：
https://blog.csdn.net/qq_37541097/article/details/81214953

* Yolov3&Yolov4&Yolov5       
https://zhuanlan.zhihu.com/p/143747206

* Faster rcnn   
https://zhuanlan.zhihu.com/p/31426458   
https://www.cnblogs.com/dudumiaomiao/p/6560841.html

* SSD模型详解   
https://blog.csdn.net/baidu_41848695/article/details/100023053

* SSD与Yolo的区别：   
https://blog.csdn.net/BlowfishKing/article/details/80485006

* Faster rcnn、SSD和yolo系列的正负样本标定   
https://blog.csdn.net/xiaotian127/article/details/104661466
https://zhuanlan.zhihu.com/p/138824387

* YOLOv2、v3使用K-means聚类计算anchor boxes的具体方法
https://blog.csdn.net/fu18946764506/article/details/89485493

* FPN的特征在不同层怎么处理的？    
   https://blog.csdn.net/weixin_40683960/article/details/79055537

* 令人拍案称奇的Mask RCNN    
https://zhuanlan.zhihu.com/p/37998710
https://blog.csdn.net/wangdongwei0/article/details/83110305

* Cascade rcnn     
   https://zhuanlan.zhihu.com/p/42553957

* ROI pooling / ROI Align / ROI Wraping  
https://www.cnblogs.com/wangyong/p/8523814.html

* ROI align的反向传播？
   https://blog.csdn.net/thisiszdy/article/details/89058768
   https://zhuanlan.zhihu.com/p/73138740

* FCOS的FPN分配公式、CenterNess的作用？   
  https://zhuanlan.zhihu.com/p/63868458       
https://blog.csdn.net/WZZ18191171661/article/details/89258086

* CornerNet    
https://blog.csdn.net/u014380165/article/details/83032273

* CenterNet   
https://zhuanlan.zhihu.com/p/66048276   

* Cornernet/Centernet代码里面GT heatmap   
https://zhuanlan.zhihu.com/p/96856635

* RepPoints    
https://zhuanlan.zhihu.com/p/64522910

* EfficientDet  
https://zhuanlan.zhihu.com/p/129776902

* End-to-End Object Detection with Transformers-DETR    
https://zhuanlan.zhihu.com/p/144974069

* 目标检测番外篇(2)_mAP   
https://zhuanlan.zhihu.com/p/48992451
----
#### NLP
* 完全图解RNN、RNN变体、Seq2Seq、Attention机制         
https://zhuanlan.zhihu.com/p/28054589

* 一文搞懂RNN（循环神经网络）基础篇   
https://zhuanlan.zhihu.com/p/30844905


----
#### 机器学习
* 决策树(Decision Tree)   
https://zhuanlan.zhihu.com/p/30059442

* PCA   
https://www.zhihu.com/question/41120789/answer/481966094

* 零基础学SVM    
https://zhuanlan.zhihu.com/p/24638007

* kmeans   
https://zhuanlan.zhihu.com/p/75477709

* LR逻辑回归   
https://zhuanlan.zhihu.com/p/73608677

* 随机森林   
https://www.zhihu.com/question/64043740/answer/644998828

* 集成学习（Ensemble Learning）  
https://www.cnblogs.com/zongfa/p/9304353.html

* 八种常见机器学习对比     
https://blog.csdn.net/Mason_Mao/article/details/82693701

* ROC曲线   AUC
https://blog.csdn.net/qq_30992103/article/details/99730059
https://www.bioinfo-scrounger.com/archives/767/

----
#### Pytorch
* Pytorch详解NLLLoss和CrossEntropyLoss：          
https://blog.csdn.net/qq_22210253/article/details/85229988  
https://github.com/mepeichun/Efficient-Neural-Network-Bilibili/blob/master/4-Knowledge-Distillation

* Pytorch中BCEloss, BCEwithlogitsloss的区别：
https://blog.csdn.net/qq_22210253/article/details/85222093

* Pytorch .detach的作用：  
https://blog.csdn.net/qq_39709535/article/details/80804003

* Pytorch Hook    
https://zhuanlan.zhihu.com/p/75054200

* pytorch中的广播机制   
```
1. A.ndim > B.ndim, 并且A.shape最后几个元素包含B.shape, 比如下面三种情况, 注意不要混淆ndim和shape这两个基本概念
    A.shape=(2,3,4,5), B.shape=(3,4,5)
    A.shape=(2,3,4,5), B.shape=(4,5)
    A.shape=(2,3,4,5), B.shape=(5)
2. A.ndim == B.ndim, 并且A.shape和B.shape对应位置的元素要么相同要么其中一个是1, 比如
    A.shape=(1,9,4), B.shape=(15,1,4)
    A.shape=(1,9,4), B.shape=(15,1,1)

https://blog.csdn.net/littlehaes/article/details/103807303
```

* Sequential的三种写法
```
net1 = nn.Sequential()
net1.add_module('conv', nn.Conv2d(3, 3, 3))
net1.add_module('batchnorm', nn.BatchNorm2d(3))
net1.add_module('activation_layer', nn.ReLU())
 
net2 = nn.Sequential(
        nn.Conv2d(3, 3, 3),
        nn.BatchNorm2d(3),
        nn.ReLU()
        )
 
from collections import OrderedDict
net3= nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(3, 3, 3)),
          ('bn1', nn.BatchNorm2d(3)),
          ('relu1', nn.ReLU())
        ]))
```

* 调整学习率的方法
```
# 方法1: 调整学习率，新建一个optimizer
old_lr = 0.1
optimizer1 =optim.SGD([
                {'params': net.features.parameters()},
                {'params': net.classifier.parameters(), 'lr': old_lr*0.1}
            ], lr=1e-5)
 
# 方法2: 调整学习率, 手动decay, 保存动量
for param_group in optimizer.param_groups:
    param_group['lr'] *= 0.1 # 学习率为之前的0.1倍
```

* nn.functional中的函数和nn.Module主要区别：
```
1. nn.Module实现的layers是一个特殊的类，都是有class layer(nn.Module)定义，会自动提取可学习的参数
2. nn.functional中的函数更像是纯函数，由def function(input)定义
3. 也就是说如果模型有可学习的参数，最好用nn.Module否则使用哪个都可以，二者在性能上没多大差异，
4. 对于卷积，全连接等具有可学习参数的网络建议使用nn.Module
5. 激活函数（ReLU,sigmoid,tanh），池化等可以使用functional替代。对于不具有可学习参数的层，将他们用函数代替，这样可以不用放在构造函数__init__中。
```

* pytorch多卡训练的原理    
    https://blog.csdn.net/wyz6666/article/details/99484326

```
（1）将模型加载到一个指定的主GPU上，然后将模型浅拷贝到其它的从GPU上；
（2）将总的batch数据等分到不同的GPU上（坑：需要先将数据加载到主GPU上）；
（3）每个GPU根据自己分配到的数据进行forward计算得到loss，并通过backward得到权重梯度；
（4）主GPU将所有从GPU得到的梯度进行合并并用于更新模型的参数。
    https://blog.csdn.net/wyz6666/article/details/99484326
```

* pytorch中train和eval有什么不同
```
(1). model.train()——训练时候启用
启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为True
(2). model.eval()——验证和测试时候启用
不启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为False

train模式会计算梯度，eval模式不会计算梯度。
```
* DataLoader, DataSet, Sampler之间的关系
https://www.cnblogs.com/marsggbo/p/11308889.html

-----
#### 编程语言（Python）
* python 深拷贝浅拷贝：   
https://blog.csdn.net/zhubaoJay/article/details/90897028  
https://www.csdn.net/gather_2f/MtjaIgysMjQwLWJsb2cO0O0O.html

* python 序列化与反序列化   
https://www.cnblogs.com/wangchunli-blogs/p/9949671.html

* python垃圾回收机制   
```
python采用的是引用计数机制为主，标记-清除和分代收集(隔代回收、分代回收)两种机制为辅的策略
计数机制
Python的GC模块主要运用了引用计数来跟踪和回收垃圾。
在引用计数的基础上，还可以通过“标记-清除”解决容器对象可能产生的循环引用的问题。通过分代回收以空间换取时间进一步提高垃圾回收的效率。
标记-清除：
标记-清除的出现打破了循环引用，也就是它只关注那些可能会产生循环引用的对象
缺点：该机制所带来的额外操作和需要回收的内存块成正比。
隔代回收
原理：将系统中的所有内存块根据其存活时间划分为不同的集合，每一个集合就成为一个“代”，垃圾收集的频率随着“代”的存活时间的增大而减小。也就是说，活得越长的对象，就越不可能是垃圾，就应该减少对它的垃圾收集频率。那么如何来衡量这个存活时间：通常是利用几次垃圾收集动作来衡量，如果一个对象经过的垃圾收集次数越多，可以得出：该对象存活时间就越长。
```

* GIL全局解释器锁
```
在Cpython解释器才有GIL的概念，不是python的特点。
python在设计的时候，还没有多核的概念。因此，为了设计方便与线程安全，直接设计了一个锁：GIL锁
在一个进程下，一次只能有一个线程执行，以此来保证数据的安全性。
从这也可以看出，为多线程分配多个CPU，多个CPU也不会起作用，因为每次只能执行一个线程。所以python中的线程只能实现并发，不能实现真正的并行。
```

* is和==的区别？
```
is:判断内存地址是否相等
==：判断数值是否相等
```

* python中闭包，闭包的实质   
https://www.jianshu.com/p/5582ca53d53e


* 解释继承
```
一个类继承自另一个类，也可以说是一个孩子类/派生类/子类，继承自父类/基类/超类，同时获取所有的类成员（属性和方法）。
继承使我们可以重用代码，并且还可以更方便地创建和维护代码。Python 支持以下类型的继承：
单继承- 一个子类类继承自单个基类
多重继承- 一个子类继承自多个基类
多级继承- 一个子类继承自一个基类，而基类继承自另一个基类
分层继承- 多个子类继承自同一个基类
混合继承- 两种或两种以上继承类型的组合
```

* 迭代器和生成器的区别
```
迭代器是一个更加抽象的概念，任何对象，如果它的类有next方法和iter方法返回自身。对于string、list、dict、tuple等这类容器对象，使用for循环遍历是很方便的。在后台for语句对容器对象调用iter()函数，iter()是Python的内置函数。iter()会返回一个定义了next()方法的迭代器对象，它在容器中逐个访问容器内元素，next()也是python的内置函数。在没有后续元素时，next()会抛出一个StopIterration的异常。
生成器（Generator）是创建迭代器的简单而强大的工具。它们写起来就像是正规的函数，只是在返回数据的时候需要使用yield语句。每次next()被调用时，生成器会返回它脱离的位置（它记忆语句最后一次执行的位置和所有的数据值）
```

* python 装饰器  
https://www.zhihu.com/question/26930016/answer/1047233982

* python中关于unicode,utf-8,gbk等编码   
https://blog.csdn.net/feiyang5260/article/details/81947444

* Python中*args、**args      
https://blog.csdn.net/qq_41877039/article/details/97623476
-----
#### 编程语言（C++）
* C++三大特性之多态      
https://blog.csdn.net/skySongkran/article/details/82012698

* C++三大特性   
https://blog.csdn.net/qq_43414142/article/details/100892336

* C++ 静多态与动多态       
https://www.cnblogs.com/staring-hxs/p/3669497.html

* C++继承中重载、重写、重定义、虚函数  
https://blog.csdn.net/AndyYoung77/article/details/90146893

* C/C++ 全局变量和局部变量在内存里的区别    
https://blog.csdn.net/luke_sanjayzzzhong/article/details/102469462

-----
#### 常见代码
* 目标检测常用代码  
https://github.com/miaoshuyu/object-detection-usages
